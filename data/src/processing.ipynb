{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "collisions_df = pd.read_csv(\"../../input/collisions.csv\")\n",
    "geometry_df = gpd.read_file(\"../../input/taxi_zones/taxi_zones.shp\")\n",
    "collisions_df[\"crash_date\"] = pd.to_datetime(collisions_df[\"crash_date\"])\n",
    "collisions_df = collisions_df[collisions_df['crash_date'].dt.year == 2024]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../input/yellow_taxi_data/yellow_tripdata_2024-04.parquet: original=3,514,289, sampled=500,000\n",
      "../../input/yellow_taxi_data/yellow_tripdata_2024-05.parquet: original=3,723,833, sampled=500,000\n",
      "../../input/yellow_taxi_data/yellow_tripdata_2024-07.parquet: original=3,076,903, sampled=500,000\n",
      "../../input/yellow_taxi_data/yellow_tripdata_2024-06.parquet: original=3,539,193, sampled=500,000\n",
      "../../input/yellow_taxi_data/yellow_tripdata_2024-03.parquet: original=3,582,628, sampled=500,000\n",
      "../../input/yellow_taxi_data/yellow_tripdata_2024-02.parquet: original=3,007,526, sampled=500,000\n",
      "../../input/yellow_taxi_data/yellow_tripdata_2024-12.parquet: original=3,668,371, sampled=500,000\n",
      "../../input/yellow_taxi_data/yellow_tripdata_2024-09.parquet: original=3,633,030, sampled=500,000\n",
      "../../input/yellow_taxi_data/yellow_tripdata_2024-10.parquet: original=3,833,771, sampled=500,000\n",
      "../../input/yellow_taxi_data/yellow_tripdata_2024-01.parquet: original=2,964,624, sampled=500,000\n",
      "../../input/yellow_taxi_data/yellow_tripdata_2024-11.parquet: original=3,646,369, sampled=500,000\n",
      "../../input/yellow_taxi_data/yellow_tripdata_2024-08.parquet: original=2,979,183, sampled=500,000\n",
      "\n",
      "✅ Final combined dataframe: 6,000,000 rows\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Find all 2024 parquet files\n",
    "parquet_files = glob.glob(\"../../input/yellow_taxi_data/*2024*.parquet\")\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "# Step 2: Load, convert datetime, and sample from each file\n",
    "for file in parquet_files:\n",
    "    df = pd.read_parquet(file)\n",
    "    df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'], errors='coerce')\n",
    "    \n",
    "    sampled_df = df.sample(n=500_000, random_state=100_000)\n",
    "    \n",
    "    print(f\"{file}: original={len(df):,}, sampled={len(sampled_df):,}\")\n",
    "    all_dfs.append(sampled_df)\n",
    "\n",
    "# Step 3: Concatenate all sampled dataframes\n",
    "taxi_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "print(f\"\\n✅ Final combined dataframe: {len(taxi_df):,} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1200000 entries, 0 to 1199999\n",
      "Data columns (total 18 columns):\n",
      " #   Column                 Non-Null Count    Dtype         \n",
      "---  ------                 --------------    -----         \n",
      " 0   VendorID               1200000 non-null  int32         \n",
      " 1   tpep_pickup_datetime   1200000 non-null  datetime64[us]\n",
      " 2   tpep_dropoff_datetime  1200000 non-null  datetime64[us]\n",
      " 3   passenger_count        1082251 non-null  float64       \n",
      " 4   trip_distance          1200000 non-null  float64       \n",
      " 5   RatecodeID             1082251 non-null  float64       \n",
      " 6   PULocationID           1200000 non-null  int32         \n",
      " 7   DOLocationID           1200000 non-null  int32         \n",
      " 8   payment_type           1200000 non-null  int64         \n",
      " 9   fare_amount            1200000 non-null  float64       \n",
      " 10  extra                  1200000 non-null  float64       \n",
      " 11  mta_tax                1200000 non-null  float64       \n",
      " 12  tip_amount             1200000 non-null  float64       \n",
      " 13  tolls_amount           1200000 non-null  float64       \n",
      " 14  improvement_surcharge  1200000 non-null  float64       \n",
      " 15  total_amount           1200000 non-null  float64       \n",
      " 16  congestion_surcharge   1082251 non-null  float64       \n",
      " 17  Airport_fee            1082251 non-null  float64       \n",
      "dtypes: datetime64[us](2), float64(12), int32(3), int64(1)\n",
      "memory usage: 151.1 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>Airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-04-24 14:58:27</td>\n",
       "      <td>2024-04-24 15:28:12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.90</td>\n",
       "      <td>1.0</td>\n",
       "      <td>148</td>\n",
       "      <td>138</td>\n",
       "      <td>1</td>\n",
       "      <td>43.60</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.10</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-04-10 22:23:05</td>\n",
       "      <td>2024-04-10 22:43:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>143</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-04-03 15:14:00</td>\n",
       "      <td>2024-04-03 15:33:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>166</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>17.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-04-20 10:01:51</td>\n",
       "      <td>2024-04-20 10:23:38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>21.90</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>6.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.40</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-04-18 16:17:08</td>\n",
       "      <td>2024-04-18 16:42:25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1.0</td>\n",
       "      <td>229</td>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "      <td>22.60</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.46</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         1  2024-04-24 14:58:27   2024-04-24 15:28:12              1.0   \n",
       "1         2  2024-04-10 22:23:05   2024-04-10 22:43:14              NaN   \n",
       "2         2  2024-04-03 15:14:00   2024-04-03 15:33:00              NaN   \n",
       "3         1  2024-04-20 10:01:51   2024-04-20 10:23:38              1.0   \n",
       "4         2  2024-04-18 16:17:08   2024-04-18 16:42:25              1.0   \n",
       "\n",
       "   trip_distance  RatecodeID  PULocationID  DOLocationID  payment_type  \\\n",
       "0           9.90         1.0           148           138             1   \n",
       "1           3.70         NaN           143           113             0   \n",
       "2           2.61         NaN           166            42             0   \n",
       "3           4.60         1.0            41            68             1   \n",
       "4           2.90         1.0           229           238             1   \n",
       "\n",
       "   fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0        43.60    7.5      0.5       10.50           0.0   \n",
       "1        -1.36    0.0      0.5        0.00           0.0   \n",
       "2        17.95    0.0      0.5        0.00           0.0   \n",
       "3        21.90    2.5      0.5        6.50           0.0   \n",
       "4        22.60    2.5      0.5        4.36           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  congestion_surcharge  Airport_fee  \n",
       "0                    1.0         63.10                   2.5          0.0  \n",
       "1                    1.0          2.64                   NaN          NaN  \n",
       "2                    1.0         19.45                   NaN          NaN  \n",
       "3                    1.0         32.40                   2.5          0.0  \n",
       "4                    1.0         33.46                   2.5          0.0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_df.drop(columns=['store_and_fwd_flag'], inplace=True)\n",
    "\n",
    "taxi_df.info()\n",
    "taxi_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df.replace('NULL', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions_df.replace('NULL', np.nan, inplace=True)\n",
    "collisions_df.drop(columns=['location'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 91264 entries, 200490 to 291753\n",
      "Data columns (total 19 columns):\n",
      " #   Column                         Non-Null Count  Dtype         \n",
      "---  ------                         --------------  -----         \n",
      " 0   crash_date                     91264 non-null  datetime64[ns]\n",
      " 1   crash_time                     91264 non-null  object        \n",
      " 2   latitude                       84262 non-null  float64       \n",
      " 3   longitude                      84262 non-null  float64       \n",
      " 4   on_street_name                 65077 non-null  object        \n",
      " 5   number_of_persons_injured      91264 non-null  int64         \n",
      " 6   number_of_persons_killed       91264 non-null  int64         \n",
      " 7   number_of_pedestrians_injured  91264 non-null  int64         \n",
      " 8   number_of_pedestrians_killed   91264 non-null  int64         \n",
      " 9   number_of_cyclist_injured      91264 non-null  int64         \n",
      " 10  number_of_cyclist_killed       91264 non-null  int64         \n",
      " 11  number_of_motorist_injured     91264 non-null  int64         \n",
      " 12  number_of_motorist_killed      91264 non-null  int64         \n",
      " 13  contributing_factor_vehicle_1  90586 non-null  object        \n",
      " 14  collision_id                   91264 non-null  int64         \n",
      " 15  borough                        65101 non-null  object        \n",
      " 16  zip_code                       65081 non-null  float64       \n",
      " 17  cross_street_name              26187 non-null  object        \n",
      " 18  off_street_name                46722 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(3), int64(9), object(6)\n",
      "memory usage: 13.9+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_78200/3394947211.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  collisions_df['contributing_factor_vehicle_1'].replace('Unspecified', np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "cols = ['crash_date', 'crash_time', 'latitude', 'longitude',\n",
    "       'on_street_name', 'number_of_persons_injured',\n",
    "       'number_of_persons_killed', 'number_of_pedestrians_injured',\n",
    "       'number_of_pedestrians_killed', 'number_of_cyclist_injured',\n",
    "       'number_of_cyclist_killed', 'number_of_motorist_injured',\n",
    "       'number_of_motorist_killed', 'contributing_factor_vehicle_1', 'collision_id','borough', 'zip_code',\n",
    "       'cross_street_name', 'off_street_name']\n",
    "collisions_df = collisions_df[cols]\n",
    "collisions_df.info()\n",
    "collisions_df.head()\n",
    "\n",
    "# Replace 'Unspecified' in 'contributing_factor_vehicle_1' with NaN\n",
    "collisions_df['contributing_factor_vehicle_1'].replace('Unspecified', np.nan, inplace=True)\n",
    "\n",
    "# Drop rows where 'latitude' or 'longitude' are missing\n",
    "collisions_df.dropna(subset=['latitude', 'longitude'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crash_date: 84262 non-null values, 321 unique values\n",
      " → Column is not purely numeric.\n",
      " → crash_date has duplicate values and could be normalized.\n",
      "--------------------------------------------------\n",
      "crash_time: 84262 non-null values, 1440 unique values\n",
      " → Column is not purely numeric.\n",
      " → crash_time has duplicate values and could be normalized.\n",
      "--------------------------------------------------\n",
      "on_street_name: 58601 non-null values, 5179 unique values\n",
      " → Column is not purely numeric.\n",
      " → on_street_name has duplicate values and could be normalized.\n",
      "--------------------------------------------------\n",
      "contributing_factor_vehicle_1: 62351 non-null values, 54 unique values\n",
      " → Column is not purely numeric.\n",
      " → contributing_factor_vehicle_1 has duplicate values and could be normalized.\n",
      "--------------------------------------------------\n",
      "borough: 64358 non-null values, 5 unique values\n",
      " → Column is not purely numeric.\n",
      " → borough has duplicate values and could be normalized.\n",
      "--------------------------------------------------\n",
      "cross_street_name: 25661 non-null values, 18027 unique values\n",
      " → Column is not purely numeric.\n",
      " → cross_street_name has duplicate values and could be normalized.\n",
      "--------------------------------------------------\n",
      "off_street_name: 44293 non-null values, 5974 unique values\n",
      " → Column is not purely numeric.\n",
      " → off_street_name has duplicate values and could be normalized.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df = collisions_df\n",
    "for col in df.columns:\n",
    "    non_null_count = df[col].notnull().sum()\n",
    "    unique_count = df[col].nunique(dropna=True)\n",
    "    is_numeric = pd.api.types.is_numeric_dtype(df[col])\n",
    "\n",
    "    # Skip numeric columns\n",
    "    if is_numeric:\n",
    "        continue\n",
    "\n",
    "    # Print only if there are duplicate values (i.e., normalization candidates)\n",
    "    if unique_count < non_null_count:\n",
    "        print(f\"{col}: {non_null_count} non-null values, {unique_count} unique values\")\n",
    "        print(\" → Column is not purely numeric.\")\n",
    "        print(f\" → {col} has duplicate values and could be normalized.\")\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Create the lookup tables with IDs\n",
    "def create_lookup_table(df, column_name, new_column_name):\n",
    "    lookup_df = df[[column_name]].dropna().drop_duplicates().reset_index(drop=True)\n",
    "    lookup_df.insert(0, f\"{new_column_name}_id\", range(1, len(lookup_df) + 1))\n",
    "    return lookup_df\n",
    "\n",
    "contributing_factor_lut = create_lookup_table(collisions_df, 'contributing_factor_vehicle_1', 'contributing_factor')\n",
    "borough_lut = create_lookup_table(collisions_df, 'borough', 'borough')\n",
    "cross_street_lut = create_lookup_table(collisions_df, 'cross_street_name', 'cross_street')\n",
    "off_street_lut = create_lookup_table(collisions_df, 'off_street_name', 'off_street')\n",
    "\n",
    "# 2. Map the original columns to their IDs\n",
    "collisions_df = collisions_df.merge(contributing_factor_lut, how='left', left_on='contributing_factor_vehicle_1', right_on='contributing_factor_vehicle_1')\n",
    "collisions_df = collisions_df.merge(borough_lut, how='left', left_on='borough', right_on='borough')\n",
    "collisions_df = collisions_df.merge(cross_street_lut, how='left', left_on='cross_street_name', right_on='cross_street_name')\n",
    "collisions_df = collisions_df.merge(off_street_lut, how='left', left_on='off_street_name', right_on='off_street_name')\n",
    "\n",
    "# 3. Optional: Drop original text columns and rename *_id columns\n",
    "collisions_df = collisions_df.drop(columns=[\n",
    "    'contributing_factor_vehicle_1',\n",
    "    'borough',\n",
    "    'cross_street_name',\n",
    "    'off_street_name'\n",
    "])\n",
    "\n",
    "collisions_df = collisions_df.rename(columns={\n",
    "    'contributing_factor_id': 'contributing_factor_vehicle_1_id',\n",
    "    'borough_id': 'borough_id',\n",
    "    'cross_street_id': 'cross_street_name_id',\n",
    "    'off_street_id': 'off_street_name_id'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>Shape_Leng</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>zone</th>\n",
       "      <th>LocationID</th>\n",
       "      <th>borough</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.116357</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>1</td>\n",
       "      <td>EWR</td>\n",
       "      <td>POLYGON ((933100.918 192536.086, 933091.011 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.433470</td>\n",
       "      <td>0.004866</td>\n",
       "      <td>Jamaica Bay</td>\n",
       "      <td>2</td>\n",
       "      <td>Queens</td>\n",
       "      <td>MULTIPOLYGON (((1033269.244 172126.008, 103343...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.084341</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>Allerton/Pelham Gardens</td>\n",
       "      <td>3</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>POLYGON ((1026308.77 256767.698, 1026495.593 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.043567</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>Alphabet City</td>\n",
       "      <td>4</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>POLYGON ((992073.467 203714.076, 992068.667 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.092146</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>Arden Heights</td>\n",
       "      <td>5</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>POLYGON ((935843.31 144283.336, 936046.565 144...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>259</td>\n",
       "      <td>0.126750</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>Woodlawn/Wakefield</td>\n",
       "      <td>259</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>POLYGON ((1025414.782 270986.139, 1025138.624 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>260</td>\n",
       "      <td>0.133514</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>Woodside</td>\n",
       "      <td>260</td>\n",
       "      <td>Queens</td>\n",
       "      <td>POLYGON ((1011466.966 216463.005, 1011545.889 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>261</td>\n",
       "      <td>0.027120</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>World Trade Center</td>\n",
       "      <td>261</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>POLYGON ((980555.204 196138.486, 980570.792 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>262</td>\n",
       "      <td>0.049064</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>Yorkville East</td>\n",
       "      <td>262</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>MULTIPOLYGON (((999804.795 224498.527, 999824....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>263</td>\n",
       "      <td>0.037017</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>Yorkville West</td>\n",
       "      <td>263</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>POLYGON ((997493.323 220912.386, 997355.264 22...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>263 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     OBJECTID  Shape_Leng  Shape_Area                     zone  LocationID  \\\n",
       "0           1    0.116357    0.000782           Newark Airport           1   \n",
       "1           2    0.433470    0.004866              Jamaica Bay           2   \n",
       "2           3    0.084341    0.000314  Allerton/Pelham Gardens           3   \n",
       "3           4    0.043567    0.000112            Alphabet City           4   \n",
       "4           5    0.092146    0.000498            Arden Heights           5   \n",
       "..        ...         ...         ...                      ...         ...   \n",
       "258       259    0.126750    0.000395       Woodlawn/Wakefield         259   \n",
       "259       260    0.133514    0.000422                 Woodside         260   \n",
       "260       261    0.027120    0.000034       World Trade Center         261   \n",
       "261       262    0.049064    0.000122           Yorkville East         262   \n",
       "262       263    0.037017    0.000066           Yorkville West         263   \n",
       "\n",
       "           borough                                           nyc_geometry  \n",
       "0              EWR  POLYGON ((933100.918 192536.086, 933091.011 19...  \n",
       "1           Queens  MULTIPOLYGON (((1033269.244 172126.008, 103343...  \n",
       "2            Bronx  POLYGON ((1026308.77 256767.698, 1026495.593 2...  \n",
       "3        Manhattan  POLYGON ((992073.467 203714.076, 992068.667 20...  \n",
       "4    Staten Island  POLYGON ((935843.31 144283.336, 936046.565 144...  \n",
       "..             ...                                                ...  \n",
       "258          Bronx  POLYGON ((1025414.782 270986.139, 1025138.624 ...  \n",
       "259         Queens  POLYGON ((1011466.966 216463.005, 1011545.889 ...  \n",
       "260      Manhattan  POLYGON ((980555.204 196138.486, 980570.792 19...  \n",
       "261      Manhattan  MULTIPOLYGON (((999804.795 224498.527, 999824....  \n",
       "262      Manhattan  POLYGON ((997493.323 220912.386, 997355.264 22...  \n",
       "\n",
       "[263 rows x 7 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contributing_factor_lut\n",
    "borough_lut\n",
    "cross_street_lut\n",
    "off_street_lut\n",
    "collisions_df\n",
    "taxi_df\n",
    "geometry_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to PostgreSQL RDS instance successfully!\n",
      "Database version: ('PostgreSQL 17.2 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 12.4.0, 64-bit',)\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "RDS_HOST = os.getenv(\"DB_HOST\")\n",
    "RDS_PORT = os.getenv(\"DB_PORT\")\n",
    "RDS_USER = os.getenv(\"DB_USER\")\n",
    "RDS_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "RDS_DB = os.getenv(\"DB_NAME\")\n",
    "\n",
    "try:\n",
    "    # Establish PostgreSQL connection\n",
    "    conn = psycopg2.connect(\n",
    "        host=RDS_HOST,\n",
    "        user=RDS_USER,\n",
    "        password=RDS_PASSWORD,\n",
    "        dbname=RDS_DB,\n",
    "        port=RDS_PORT,\n",
    "        sslmode=\"require\"  \n",
    "    )\n",
    "    print(\"✅ Connected to PostgreSQL RDS instance successfully!\")\n",
    "\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execute a test query\n",
    "    cursor.execute(\"SELECT version();\")\n",
    "    version = cursor.fetchone()\n",
    "    print(\"Database version:\", version)\n",
    "\n",
    "    # Close connection\n",
    "    # cursor.close()\n",
    "    # conn.close()\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error connecting to RDS:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_from_df(df, table_name, cursor):\n",
    "    # Generate column definitions\n",
    "    columns = []\n",
    "    for col_name, dtype in zip(df.columns, df.dtypes):\n",
    "        if 'int' in str(dtype):\n",
    "            col_type = 'INTEGER'\n",
    "        elif 'float' in str(dtype):\n",
    "            col_type = 'NUMERIC'\n",
    "        elif 'datetime' in str(dtype):\n",
    "            col_type = 'TIMESTAMP'\n",
    "        elif 'bool' in str(dtype):\n",
    "            col_type = 'BOOLEAN'\n",
    "        else:\n",
    "            col_type = 'TEXT'\n",
    "        \n",
    "        columns.append(f'\"{col_name}\" {col_type}')\n",
    "    \n",
    "    # Create table\n",
    "    create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({', '.join(columns)})\"\n",
    "    cursor.execute(create_table_query)\n",
    "    print(f\"Created table {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_df_to_postgres(df, table_name, cursor):\n",
    "    from io import StringIO\n",
    "    import csv  # Import csv for quoting options\n",
    "\n",
    "    # Create a buffer\n",
    "    buffer = StringIO()\n",
    "\n",
    "    # Write the DataFrame to the buffer\n",
    "    df.to_csv(buffer, index=False, header=False, na_rep='', quoting=csv.QUOTE_MINIMAL)\n",
    "    buffer.seek(0)\n",
    "\n",
    "    try:\n",
    "        # Use COPY command for fast data loading\n",
    "        column_list = ','.join([f'\"{col}\"' for col in df.columns])\n",
    "        cursor.copy_expert(\n",
    "            f\"COPY {table_name} ({column_list}) FROM STDIN WITH CSV NULL ''\",\n",
    "            buffer\n",
    "        )\n",
    "        print(f\"Uploaded {len(df)} rows to {table_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading to {table_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing taxi_df with 1200000 rows...\n",
      "Created table taxi_df\n",
      "Uploading chunk 1/24 (0 to 50000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 2/24 (50000 to 100000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 3/24 (100000 to 150000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 4/24 (150000 to 200000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 5/24 (200000 to 250000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 6/24 (250000 to 300000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 7/24 (300000 to 350000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 8/24 (350000 to 400000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 9/24 (400000 to 450000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 10/24 (450000 to 500000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 11/24 (500000 to 550000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 12/24 (550000 to 600000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 13/24 (600000 to 650000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 14/24 (650000 to 700000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 15/24 (700000 to 750000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 16/24 (750000 to 800000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 17/24 (800000 to 850000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 18/24 (850000 to 900000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 19/24 (900000 to 950000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 20/24 (950000 to 1000000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 21/24 (1000000 to 1050000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 22/24 (1050000 to 1100000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 23/24 (1100000 to 1150000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 24/24 (1150000 to 1200000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "All data uploaded successfully!\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "# List of dataframes to upload\n",
    "dataframes = {\n",
    "    'geometry_df': geometry_df,\n",
    "    'contributing_factor_lut': contributing_factor_lut,\n",
    "    'borough_lut': borough_lut,\n",
    "    'cross_street_lut': cross_street_lut,\n",
    "    'off_street_lut': off_street_lut,\n",
    "    'collisions_df': collisions_df,\n",
    "    'taxi_df': taxi_df\n",
    "}\n",
    "\n",
    "# Process each dataframe\n",
    "for table_name, df in dataframes.items():\n",
    "    print(f\"🔄 Processing {table_name} with {len(df)} rows...\")\n",
    "    \n",
    "    # Create the table\n",
    "    create_table_from_df(df, table_name, cursor)\n",
    "    \n",
    "    # Process in chunks to handle large dataframes\n",
    "    chunk_size = 50000\n",
    "    total_chunks = (len(df) + chunk_size - 1) // chunk_size  # Ceiling division\n",
    "    \n",
    "    for i in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[i:i+chunk_size]\n",
    "        chunk_num = i // chunk_size + 1\n",
    "        print(f\"Uploading chunk {chunk_num}/{total_chunks} ({i} to {min(i+chunk_size, len(df))} rows)\")\n",
    "        copy_df_to_postgres(chunk, table_name, cursor)\n",
    "        # Commit after each chunk to avoid long transactions\n",
    "        conn.commit()\n",
    "\n",
    "print(\"All data uploaded successfully!\")\n",
    "\n",
    "# Close connections\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"Connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop all tables in the public schema\n",
    "# cursor.execute(\"\"\"\n",
    "#     DO $$ \n",
    "#     BEGIN\n",
    "#         EXECUTE (\n",
    "#             SELECT string_agg('DROP TABLE IF EXISTS \"' || table_name || '\" CASCADE;', ' ')\n",
    "#             FROM information_schema.tables\n",
    "#             WHERE table_schema = 'public'\n",
    "#         );\n",
    "#     END $$;\n",
    "# \"\"\")\n",
    "# conn.commit()\n",
    "\n",
    "# print(\"All tables dropped successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "    SELECT table_name\n",
    "    FROM information_schema.tables\n",
    "    WHERE table_schema = 'public'\n",
    "\"\"\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "# Print the table names\n",
    "print(\"Tables in the database:\")\n",
    "for table in tables:\n",
    "    print(table[0])\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT COUNT(*) AS row_count\n",
    "    FROM taxi_df\n",
    "\"\"\")\n",
    "row_count = cursor.fetchone()\n",
    "\n",
    "# Print the number of rows\n",
    "print(f\"Number of rows in taxi_df: {row_count[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows already uploaded: 1200000\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"SELECT COUNT(*) FROM taxi_df\")\n",
    "uploaded_rows = cursor.fetchone()[0]\n",
    "print(f\"Rows already uploaded: {uploaded_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_row = uploaded_rows  # Start from the last uploaded row\n",
    "# chunk_size = 50000  # Adjust chunk size if needed\n",
    "\n",
    "# for i in range(start_row, len(taxi_df), chunk_size):\n",
    "#     chunk = taxi_df.iloc[i:i+chunk_size]\n",
    "#     chunk_num = i // chunk_size + 1\n",
    "#     print(f\"Uploading chunk {chunk_num} ({i} to {min(i+chunk_size, len(taxi_df))} rows)\")\n",
    "#     copy_df_to_postgres(chunk, 'taxi_df', cursor)\n",
    "#     conn.commit()  # Commit after each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command executed successfully.\n",
      "Command executed successfully.\n",
      "Command executed successfully.\n",
      "Command executed successfully.\n"
     ]
    }
   ],
   "source": [
    "# List of SQL commands to execute\n",
    "sql_commands = [\n",
    "    \"\"\"\n",
    "    CREATE TABLE borough_lut_temp AS\n",
    "    SELECT DISTINCT * FROM borough_lut;\n",
    "\n",
    "    DROP TABLE borough_lut;\n",
    "\n",
    "    ALTER TABLE borough_lut_temp RENAME TO borough_lut;\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE contributing_factor_lut_temp AS\n",
    "    SELECT DISTINCT * FROM contributing_factor_lut;\n",
    "\n",
    "    DROP TABLE contributing_factor_lut;\n",
    "\n",
    "    ALTER TABLE contributing_factor_lut_temp RENAME TO contributing_factor_lut;\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE cross_street_lut_temp AS\n",
    "    SELECT DISTINCT * FROM cross_street_lut;\n",
    "\n",
    "    DROP TABLE cross_street_lut;\n",
    "\n",
    "    ALTER TABLE cross_street_lut_temp RENAME TO cross_street_lut;\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE off_street_lut_temp AS\n",
    "    SELECT DISTINCT * FROM off_street_lut;\n",
    "\n",
    "    DROP TABLE off_street_lut;\n",
    "\n",
    "    ALTER TABLE off_street_lut_temp RENAME TO off_street_lut;\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Execute each SQL command\n",
    "for command in sql_commands:\n",
    "    try:\n",
    "        cursor.execute(command)\n",
    "        conn.commit()\n",
    "        print(\"Command executed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing command: {e}\")\n",
    "        conn.rollback()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows removed successfully.\n",
      "Duplicate rows removed successfully.\n",
      "Error adding primary key: multiple primary keys for table \"collisions_df\" are not allowed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of SQL commands to remove duplicates for each table\n",
    "sql_remove_duplicates_commands = [\n",
    "    \"\"\"\n",
    "    DELETE FROM collisions_df\n",
    "    WHERE ctid NOT IN (\n",
    "        SELECT MIN(ctid)\n",
    "        FROM collisions_df\n",
    "        GROUP BY collision_id\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    DELETE FROM nyc_geometry_df\n",
    "    WHERE ctid NOT IN (\n",
    "        SELECT MIN(ctid)\n",
    "        FROM nyc_geometry_df\n",
    "        GROUP BY \"LocationID\"\n",
    "    );\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Execute each SQL command\n",
    "for command in sql_remove_duplicates_commands:\n",
    "    try:\n",
    "        cursor.execute(command)\n",
    "        conn.commit()\n",
    "        print(\"Duplicate rows removed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error removing duplicates: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "# Add primary keys to the tables\n",
    "sql_add_primary_keys = [\n",
    "    \"ALTER TABLE borough_lut ADD PRIMARY KEY (borough_id);\",\n",
    "    \"ALTER TABLE contributing_factor_lut ADD PRIMARY KEY (contributing_factor_id);\",\n",
    "    \"ALTER TABLE cross_street_lut ADD PRIMARY KEY (cross_street_id);\",\n",
    "    \"ALTER TABLE off_street_lut ADD PRIMARY KEY (off_street_id);\",\n",
    "    \"ALTER TABLE collisions_df ADD PRIMARY KEY (collision_id);\",\n",
    "    \"ALTER TABLE nyc_geometry_df ADD PRIMARY KEY (\\\"LocationID\\\");\",\n",
    "    \"ALTER TABLE taxi_df ADD COLUMN trip_id SERIAL PRIMARY KEY;\"\n",
    "]\n",
    "\n",
    "# Execute each SQL command to add primary keys\n",
    "for command in sql_add_primary_keys:\n",
    "    try:\n",
    "        cursor.execute(command)\n",
    "        conn.commit()\n",
    "        print(\"Primary key added successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error adding primary key: {e}\")\n",
    "        conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of SQL commands to alter column types\n",
    "sql_alter_columns = [\n",
    "    \"\"\"\n",
    "    ALTER TABLE collisions_df\n",
    "    ALTER COLUMN borough_id TYPE integer USING borough_id::integer;\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    ALTER TABLE collisions_df\n",
    "    ALTER COLUMN contributing_factor_vehicle_1_id TYPE integer USING contributing_factor_vehicle_1_id::integer;\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    ALTER TABLE collisions_df\n",
    "    ALTER COLUMN cross_street_name_id TYPE integer USING cross_street_name_id::integer;\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    ALTER TABLE collisions_df\n",
    "    ALTER COLUMN off_street_name_id TYPE integer USING off_street_name_id::integer;\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Execute each SQL command to alter column types\n",
    "for command in sql_alter_columns:\n",
    "    try:\n",
    "        cursor.execute(command)\n",
    "        conn.commit()\n",
    "        print(\"Column type altered successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error altering column type: {e}\")\n",
    "        conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of SQL commands to rename tables\n",
    "sql_rename_tables = [\n",
    "    \"ALTER TABLE collisions_df RENAME TO collision;\",\n",
    "    \"ALTER TABLE taxi_df RENAME TO taxi;\",\n",
    "    \"ALTER TABLE nyc_geometry_df RENAME TO nyc_geometry;\"\n",
    "]\n",
    "\n",
    "# Execute each SQL command to rename tables\n",
    "for command in sql_rename_tables:\n",
    "    try:\n",
    "        cursor.execute(command)\n",
    "        conn.commit()\n",
    "        print(\"Table renamed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error renaming table: {e}\")\n",
    "        conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add foreign key constraints\n",
    "sql_add_foreign_keys = [\n",
    "    \"\"\"\n",
    "    ALTER TABLE taxi_df\n",
    "    ADD CONSTRAINT fk_taxi_pickup_location\n",
    "    FOREIGN KEY (PULocationID)\n",
    "    REFERENCES nyc_geometry_df (LocationID)\n",
    "    NOT VALID;\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    ALTER TABLE taxi_df\n",
    "    ADD CONSTRAINT fk_taxi_dropoff_location\n",
    "    FOREIGN KEY (DOLocationID)\n",
    "    REFERENCES nyc_geometry_df (LocationID)\n",
    "    NOT VALID;\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    ALTER TABLE collisions_df\n",
    "    ADD CONSTRAINT fk_collision_borough\n",
    "    FOREIGN KEY (borough_id)\n",
    "    REFERENCES borough_lut (borough_id);\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    ALTER TABLE collisions_df\n",
    "    ADD CONSTRAINT fk_collision_contributing_factor\n",
    "    FOREIGN KEY (contributing_factor_id)\n",
    "    REFERENCES contributing_factor_lut (contributing_factor_id);\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    ALTER TABLE collisions_df\n",
    "    ADD CONSTRAINT fk_collision_cross_street\n",
    "    FOREIGN KEY (cross_street_id)\n",
    "    REFERENCES cross_street_lut (cross_street_id);\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    ALTER TABLE collisions_df\n",
    "    ADD CONSTRAINT fk_collision_off_street\n",
    "    FOREIGN KEY (off_street_id)\n",
    "    REFERENCES off_street_lut (off_street_id);\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Execute each SQL command to add foreign keys\n",
    "for command in sql_add_foreign_keys:\n",
    "    try:\n",
    "        cursor.execute(command)\n",
    "        conn.commit()\n",
    "        print(\"Foreign key added successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error adding foreign key: {e}\")\n",
    "        conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Renaming nyc_geometry.LocationID → locationid\n",
      "-- Renaming nyc_geometry.Shape_Leng → shape_leng\n",
      "-- Renaming nyc_geometry.Shape_Area → shape_area\n",
      "-- Renaming nyc_geometry.OBJECTID → objectid\n",
      "-- Renaming taxi.VendorID → vendorid\n",
      "-- Renaming taxi.RatecodeID → ratecodeid\n",
      "-- Renaming taxi.PULocationID → pulocationid\n",
      "-- Renaming taxi.DOLocationID → dolocationid\n",
      "-- Renaming taxi.Airport_fee → airport_fee\n"
     ]
    }
   ],
   "source": [
    "alter_commands = [\n",
    "    \"ALTER TABLE nyc_geometry RENAME COLUMN \\\"LocationID\\\" TO location_id;\",\n",
    "    \"ALTER TABLE nyc_geometry RENAME COLUMN \\\"Shape_Leng\\\" TO shape_leng;\",\n",
    "    \"ALTER TABLE nyc_geometry RENAME COLUMN \\\"Shape_Area\\\" TO shape_area;\",\n",
    "    \"ALTER TABLE nyc_geometry RENAME COLUMN \\\"OBJECTID\\\" TO object_id;\",\n",
    "    \"ALTER TABLE taxi RENAME COLUMN \\\"VendorID\\\" TO vendor_id;\",\n",
    "    \"ALTER TABLE taxi RENAME COLUMN \\\"RatecodeID\\\" TO ratecode_id;\",\n",
    "    \"ALTER TABLE taxi RENAME COLUMN \\\"PULocationID\\\" TO pu_location_id;\",\n",
    "    \"ALTER TABLE taxi RENAME COLUMN \\\"DOLocationID\\\" TO do_location_id;\",\n",
    "    \"ALTER TABLE taxi RENAME COLUMN \\\"Airport_fee\\\" TO airport_fee;\",\n",
    "    \"\"\"UPDATE borough_lut\n",
    "    SET borough = INITCAP(borough);\"\"\"\n",
    "]\n",
    "\n",
    "# Execute each command\n",
    "for cmd in alter_commands:\n",
    "    try:\n",
    "        print(f\"Executing: {cmd}\")\n",
    "        cursor.execute(cmd)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to execute: {cmd}\\nError: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cursor.execute(\"\"\"\n",
    "#     SELECT column_name, data_type, is_nullable\n",
    "#     FROM information_schema.columns\n",
    "#     WHERE table_name = 'geometry'\n",
    "#     ORDER BY ordinal_position\n",
    "# \"\"\")\n",
    "# schema = cursor.fetchall()\n",
    "\n",
    "# print(\"Geometry Table Schema:\")\n",
    "# for column in schema:\n",
    "#     print(f\"{column[0]} — {column[1]} — {'NULLABLE' if column[2] == 'YES' else 'NOT NULL'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
