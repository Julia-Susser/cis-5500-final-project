{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "collisions_df = pd.read_csv(\"../../input/collisions.csv\")\n",
    "geometry_df = gpd.read_file(\"../../input/taxi_zones/taxi_zones.shp\")\n",
    "collisions_df[\"crash_date\"] = pd.to_datetime(collisions_df[\"crash_date\"])\n",
    "collisions_df = collisions_df[collisions_df['crash_date'].dt.year == 2024]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "parquet_files = glob.glob(\"../../input/yellow_taxi_data/*2024*.parquet\")\n",
    "all_dfs = []\n",
    "for file in parquet_files:\n",
    "    df = pd.read_parquet(file)\n",
    "    all_dfs.append(df)\n",
    "\n",
    "taxi_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "taxi_df['tpep_pickup_datetime'] = pd.to_datetime(taxi_df['tpep_pickup_datetime'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41169720 entries, 0 to 41169719\n",
      "Data columns (total 18 columns):\n",
      " #   Column                 Dtype         \n",
      "---  ------                 -----         \n",
      " 0   VendorID               int32         \n",
      " 1   tpep_pickup_datetime   datetime64[us]\n",
      " 2   tpep_dropoff_datetime  datetime64[us]\n",
      " 3   passenger_count        float64       \n",
      " 4   trip_distance          float64       \n",
      " 5   RatecodeID             float64       \n",
      " 6   PULocationID           int32         \n",
      " 7   DOLocationID           int32         \n",
      " 8   payment_type           int64         \n",
      " 9   fare_amount            float64       \n",
      " 10  extra                  float64       \n",
      " 11  mta_tax                float64       \n",
      " 12  tip_amount             float64       \n",
      " 13  tolls_amount           float64       \n",
      " 14  improvement_surcharge  float64       \n",
      " 15  total_amount           float64       \n",
      " 16  congestion_surcharge   float64       \n",
      " 17  Airport_fee            float64       \n",
      "dtypes: datetime64[us](2), float64(12), int32(3), int64(1)\n",
      "memory usage: 5.1 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>Airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-04-01 00:02:40</td>\n",
       "      <td>2024-04-01 00:30:42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>161</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>29.6</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43.25</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-04-01 00:41:12</td>\n",
       "      <td>2024-04-01 00:55:29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>1</td>\n",
       "      <td>25.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-04-01 00:48:42</td>\n",
       "      <td>2024-04-01 01:05:30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>186</td>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "      <td>20.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.60</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-04-01 00:56:02</td>\n",
       "      <td>2024-04-01 01:05:09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>137</td>\n",
       "      <td>164</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-04-01 00:08:32</td>\n",
       "      <td>2024-04-01 00:10:24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.0</td>\n",
       "      <td>236</td>\n",
       "      <td>263</td>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.10</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         1  2024-04-01 00:02:40   2024-04-01 00:30:42              0.0   \n",
       "1         2  2024-04-01 00:41:12   2024-04-01 00:55:29              1.0   \n",
       "2         2  2024-04-01 00:48:42   2024-04-01 01:05:30              1.0   \n",
       "3         2  2024-04-01 00:56:02   2024-04-01 01:05:09              1.0   \n",
       "4         1  2024-04-01 00:08:32   2024-04-01 00:10:24              1.0   \n",
       "\n",
       "   trip_distance  RatecodeID  PULocationID  DOLocationID  payment_type  \\\n",
       "0           5.20         1.0           161             7             1   \n",
       "1           5.60         1.0           264           264             1   \n",
       "2           3.55         1.0           186           236             1   \n",
       "3           1.06         1.0           137           164             2   \n",
       "4           0.70         1.0           236           263             1   \n",
       "\n",
       "   fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0         29.6    3.5      0.5        8.65           0.0   \n",
       "1         25.4    1.0      0.5       10.00           0.0   \n",
       "2         20.5    1.0      0.5        5.10           0.0   \n",
       "3         10.0    1.0      0.5        0.00           0.0   \n",
       "4          5.1    3.5      0.5        2.00           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  congestion_surcharge  Airport_fee  \n",
       "0                    1.0         43.25                   2.5          0.0  \n",
       "1                    1.0         37.90                   0.0          0.0  \n",
       "2                    1.0         30.60                   2.5          0.0  \n",
       "3                    1.0         15.00                   2.5          0.0  \n",
       "4                    1.0         12.10                   2.5          0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_df.drop(columns=['store_and_fwd_flag'], inplace=True)\n",
    "\n",
    "taxi_df.info()\n",
    "taxi_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df.replace('NULL', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions_df.replace('NULL', np.nan, inplace=True)\n",
    "collisions_df.drop(columns=['location'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 91264 entries, 200490 to 291753\n",
      "Data columns (total 19 columns):\n",
      " #   Column                         Non-Null Count  Dtype         \n",
      "---  ------                         --------------  -----         \n",
      " 0   crash_date                     91264 non-null  datetime64[ns]\n",
      " 1   crash_time                     91264 non-null  object        \n",
      " 2   latitude                       84262 non-null  float64       \n",
      " 3   longitude                      84262 non-null  float64       \n",
      " 4   on_street_name                 65077 non-null  object        \n",
      " 5   number_of_persons_injured      91264 non-null  int64         \n",
      " 6   number_of_persons_killed       91264 non-null  int64         \n",
      " 7   number_of_pedestrians_injured  91264 non-null  int64         \n",
      " 8   number_of_pedestrians_killed   91264 non-null  int64         \n",
      " 9   number_of_cyclist_injured      91264 non-null  int64         \n",
      " 10  number_of_cyclist_killed       91264 non-null  int64         \n",
      " 11  number_of_motorist_injured     91264 non-null  int64         \n",
      " 12  number_of_motorist_killed      91264 non-null  int64         \n",
      " 13  contributing_factor_vehicle_1  90586 non-null  object        \n",
      " 14  collision_id                   91264 non-null  int64         \n",
      " 15  borough                        65101 non-null  object        \n",
      " 16  zip_code                       65081 non-null  float64       \n",
      " 17  cross_street_name              26187 non-null  object        \n",
      " 18  off_street_name                46722 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(3), int64(9), object(6)\n",
      "memory usage: 13.9+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82439/3394947211.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  collisions_df['contributing_factor_vehicle_1'].replace('Unspecified', np.nan, inplace=True)\n",
      "/tmp/ipykernel_82439/3394947211.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  collisions_df['contributing_factor_vehicle_1'].replace('Unspecified', np.nan, inplace=True)\n",
      "/tmp/ipykernel_82439/3394947211.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  collisions_df.dropna(subset=['latitude', 'longitude'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "cols = ['crash_date', 'crash_time', 'latitude', 'longitude',\n",
    "       'on_street_name', 'number_of_persons_injured',\n",
    "       'number_of_persons_killed', 'number_of_pedestrians_injured',\n",
    "       'number_of_pedestrians_killed', 'number_of_cyclist_injured',\n",
    "       'number_of_cyclist_killed', 'number_of_motorist_injured',\n",
    "       'number_of_motorist_killed', 'contributing_factor_vehicle_1', 'collision_id','borough', 'zip_code',\n",
    "       'cross_street_name', 'off_street_name']\n",
    "collisions_df = collisions_df[cols]\n",
    "collisions_df.info()\n",
    "collisions_df.head()\n",
    "\n",
    "# Replace 'Unspecified' in 'contributing_factor_vehicle_1' with NaN\n",
    "collisions_df['contributing_factor_vehicle_1'].replace('Unspecified', np.nan, inplace=True)\n",
    "\n",
    "# Drop rows where 'latitude' or 'longitude' are missing\n",
    "collisions_df.dropna(subset=['latitude', 'longitude'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crash_date: 84262 non-null values, 321 unique values\n",
      " â†’ Column is not purely numeric.\n",
      " â†’ crash_date has duplicate values and could be normalized.\n",
      "--------------------------------------------------\n",
      "crash_time: 84262 non-null values, 1440 unique values\n",
      " â†’ Column is not purely numeric.\n",
      " â†’ crash_time has duplicate values and could be normalized.\n",
      "--------------------------------------------------\n",
      "location: 84262 non-null values, 40961 unique values\n",
      " â†’ Column is not purely numeric.\n",
      " â†’ location has duplicate values and could be normalized.\n",
      "--------------------------------------------------\n",
      "on_street_name: 58601 non-null values, 5179 unique values\n",
      " â†’ Column is not purely numeric.\n",
      " â†’ on_street_name has duplicate values and could be normalized.\n",
      "--------------------------------------------------\n",
      "contributing_factor_vehicle_1: 62351 non-null values, 54 unique values\n",
      " â†’ Column is not purely numeric.\n",
      " â†’ contributing_factor_vehicle_1 has duplicate values and could be normalized.\n",
      "--------------------------------------------------\n",
      "borough: 64358 non-null values, 5 unique values\n",
      " â†’ Column is not purely numeric.\n",
      " â†’ borough has duplicate values and could be normalized.\n",
      "--------------------------------------------------\n",
      "cross_street_name: 25661 non-null values, 18027 unique values\n",
      " â†’ Column is not purely numeric.\n",
      " â†’ cross_street_name has duplicate values and could be normalized.\n",
      "--------------------------------------------------\n",
      "off_street_name: 44293 non-null values, 5974 unique values\n",
      " â†’ Column is not purely numeric.\n",
      " â†’ off_street_name has duplicate values and could be normalized.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df = collisions_df\n",
    "for col in df.columns:\n",
    "    non_null_count = df[col].notnull().sum()\n",
    "    unique_count = df[col].nunique(dropna=True)\n",
    "    is_numeric = pd.api.types.is_numeric_dtype(df[col])\n",
    "\n",
    "    # Skip numeric columns\n",
    "    if is_numeric:\n",
    "        continue\n",
    "\n",
    "    # Print only if there are duplicate values (i.e., normalization candidates)\n",
    "    if unique_count < non_null_count:\n",
    "        print(f\"{col}: {non_null_count} non-null values, {unique_count} unique values\")\n",
    "        print(\" â†’ Column is not purely numeric.\")\n",
    "        print(f\" â†’ {col} has duplicate values and could be normalized.\")\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Create the lookup tables with IDs\n",
    "def create_lookup_table(df, column_name, new_column_name):\n",
    "    lookup_df = df[[column_name]].dropna().drop_duplicates().reset_index(drop=True)\n",
    "    lookup_df.insert(0, f\"{new_column_name}_id\", range(1, len(lookup_df) + 1))\n",
    "    return lookup_df\n",
    "\n",
    "contributing_factor_lut = create_lookup_table(collisions_df, 'contributing_factor_vehicle_1', 'contributing_factor')\n",
    "borough_lut = create_lookup_table(collisions_df, 'borough', 'borough')\n",
    "cross_street_lut = create_lookup_table(collisions_df, 'cross_street_name', 'cross_street')\n",
    "off_street_lut = create_lookup_table(collisions_df, 'off_street_name', 'off_street')\n",
    "\n",
    "# 2. Map the original columns to their IDs\n",
    "collisions_df = collisions_df.merge(contributing_factor_lut, how='left', left_on='contributing_factor_vehicle_1', right_on='contributing_factor_vehicle_1')\n",
    "collisions_df = collisions_df.merge(borough_lut, how='left', left_on='borough', right_on='borough')\n",
    "collisions_df = collisions_df.merge(cross_street_lut, how='left', left_on='cross_street_name', right_on='cross_street_name')\n",
    "collisions_df = collisions_df.merge(off_street_lut, how='left', left_on='off_street_name', right_on='off_street_name')\n",
    "\n",
    "# 3. Optional: Drop original text columns and rename *_id columns\n",
    "collisions_df = collisions_df.drop(columns=[\n",
    "    'contributing_factor_vehicle_1',\n",
    "    'borough',\n",
    "    'cross_street_name',\n",
    "    'off_street_name'\n",
    "])\n",
    "\n",
    "collisions_df = collisions_df.rename(columns={\n",
    "    'contributing_factor_id': 'contributing_factor_vehicle_1_id',\n",
    "    'borough_id': 'borough_id',\n",
    "    'cross_street_id': 'cross_street_name_id',\n",
    "    'off_street_id': 'off_street_name_id'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contributing_factor_lut\n",
    "borough_lut\n",
    "cross_street_lut\n",
    "off_street_lut\n",
    "collisions_df\n",
    "taxi_df\n",
    "geometry_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Connected to PostgreSQL RDS instance successfully!\n",
      "Database version: ('PostgreSQL 17.2 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 12.4.0, 64-bit',)\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "RDS_HOST = os.getenv(\"DB_HOST\")\n",
    "RDS_PORT = os.getenv(\"DB_PORT\")\n",
    "RDS_USER = os.getenv(\"DB_USER\")\n",
    "RDS_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "RDS_DB = os.getenv(\"DB_NAME\")\n",
    "\n",
    "try:\n",
    "    # Establish PostgreSQL connection\n",
    "    conn = psycopg2.connect(\n",
    "        host=RDS_HOST,\n",
    "        user=RDS_USER,\n",
    "        password=RDS_PASSWORD,\n",
    "        dbname=RDS_DB,\n",
    "        port=RDS_PORT,\n",
    "        sslmode=\"require\"  \n",
    "    )\n",
    "    print(\"âœ… Connected to PostgreSQL RDS instance successfully!\")\n",
    "\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execute a test query\n",
    "    cursor.execute(\"SELECT version();\")\n",
    "    version = cursor.fetchone()\n",
    "    print(\"Database version:\", version)\n",
    "\n",
    "    # Close connection\n",
    "    # cursor.close()\n",
    "    # conn.close()\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error connecting to RDS:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_from_df(df, table_name, cursor):\n",
    "    # Generate column definitions\n",
    "    columns = []\n",
    "    for col_name, dtype in zip(df.columns, df.dtypes):\n",
    "        if 'int' in str(dtype):\n",
    "            col_type = 'INTEGER'\n",
    "        elif 'float' in str(dtype):\n",
    "            col_type = 'NUMERIC'\n",
    "        elif 'datetime' in str(dtype):\n",
    "            col_type = 'TIMESTAMP'\n",
    "        elif 'bool' in str(dtype):\n",
    "            col_type = 'BOOLEAN'\n",
    "        else:\n",
    "            col_type = 'TEXT'\n",
    "        \n",
    "        columns.append(f'\"{col_name}\" {col_type}')\n",
    "    \n",
    "    # Create table\n",
    "    create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({', '.join(columns)})\"\n",
    "    cursor.execute(create_table_query)\n",
    "    print(f\"Created table {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_df_to_postgres(df, table_name, cursor):\n",
    "    from io import StringIO\n",
    "    import csv  # Import csv for quoting options\n",
    "\n",
    "    # Create a buffer\n",
    "    buffer = StringIO()\n",
    "\n",
    "    # Write the DataFrame to the buffer\n",
    "    df.to_csv(buffer, index=False, header=False, na_rep='', quoting=csv.QUOTE_MINIMAL)\n",
    "    buffer.seek(0)\n",
    "\n",
    "    try:\n",
    "        # Use COPY command for fast data loading\n",
    "        column_list = ','.join([f'\"{col}\"' for col in df.columns])\n",
    "        cursor.copy_expert(\n",
    "            f\"COPY {table_name} ({column_list}) FROM STDIN WITH CSV NULL ''\",\n",
    "            buffer\n",
    "        )\n",
    "        print(f\"Uploaded {len(df)} rows to {table_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading to {table_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Processing geometry_df with 263 rows...\n",
      "Created table geometry_df\n",
      "Uploading chunk 1/1 (0 to 263 rows)\n",
      "Uploaded 263 rows to geometry_df\n",
      "All data uploaded successfully!\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "# List of dataframes to upload\n",
    "dataframes = {\n",
    "    'geometry_df': geometry_df,\n",
    "    'contributing_factor_lut': contributing_factor_lut,\n",
    "    'borough_lut': borough_lut,\n",
    "    'cross_street_lut': cross_street_lut,\n",
    "    'off_street_lut': off_street_lut,\n",
    "    'collisions_df': collisions_df,\n",
    "    'taxi_df': taxi_df\n",
    "}\n",
    "\n",
    "# Process each dataframe\n",
    "for table_name, df in dataframes.items():\n",
    "    print(f\"ðŸ”„ Processing {table_name} with {len(df)} rows...\")\n",
    "    \n",
    "    # Create the table\n",
    "    create_table_from_df(df, table_name, cursor)\n",
    "    \n",
    "    # Process in chunks to handle large dataframes\n",
    "    chunk_size = 50000\n",
    "    total_chunks = (len(df) + chunk_size - 1) // chunk_size  # Ceiling division\n",
    "    \n",
    "    for i in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[i:i+chunk_size]\n",
    "        chunk_num = i // chunk_size + 1\n",
    "        print(f\"Uploading chunk {chunk_num}/{total_chunks} ({i} to {min(i+chunk_size, len(df))} rows)\")\n",
    "        copy_df_to_postgres(chunk, table_name, cursor)\n",
    "        # Commit after each chunk to avoid long transactions\n",
    "        conn.commit()\n",
    "\n",
    "print(\"All data uploaded successfully!\")\n",
    "\n",
    "# Close connections\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"Connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tables dropped successfully!\n"
     ]
    }
   ],
   "source": [
    "# Drop all tables in the public schema\n",
    "cursor.execute(\"\"\"\n",
    "    DO $$ \n",
    "    BEGIN\n",
    "        EXECUTE (\n",
    "            SELECT string_agg('DROP TABLE IF EXISTS \"' || table_name || '\" CASCADE;', ' ')\n",
    "            FROM information_schema.tables\n",
    "            WHERE table_schema = 'public'\n",
    "        );\n",
    "    END $$;\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "print(\"All tables dropped successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database:\n",
      "contributing_factor_lut\n",
      "borough_lut\n",
      "cross_street_lut\n",
      "off_street_lut\n",
      "taxi_df\n",
      "collisions_df\n",
      "Number of rows in taxi_df: 20500000\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"\"\"\n",
    "    SELECT table_name\n",
    "    FROM information_schema.tables\n",
    "    WHERE table_schema = 'public'\n",
    "\"\"\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "# Print the table names\n",
    "print(\"Tables in the database:\")\n",
    "for table in tables:\n",
    "    print(table[0])\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT COUNT(*) AS row_count\n",
    "    FROM taxi_df\n",
    "\"\"\")\n",
    "row_count = cursor.fetchone()\n",
    "\n",
    "# Print the number of rows\n",
    "print(f\"Number of rows in taxi_df: {row_count[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows already uploaded: 20500000\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"SELECT COUNT(*) FROM taxi_df\")\n",
    "uploaded_rows = cursor.fetchone()[0]\n",
    "print(f\"Rows already uploaded: {uploaded_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 411 (20500000 to 20550000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 412 (20550000 to 20600000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 413 (20600000 to 20650000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 414 (20650000 to 20700000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 415 (20700000 to 20750000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 416 (20750000 to 20800000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 417 (20800000 to 20850000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 418 (20850000 to 20900000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 419 (20900000 to 20950000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 420 (20950000 to 21000000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 421 (21000000 to 21050000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 422 (21050000 to 21100000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 423 (21100000 to 21150000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 424 (21150000 to 21200000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 425 (21200000 to 21250000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 426 (21250000 to 21300000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 427 (21300000 to 21350000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 428 (21350000 to 21400000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 429 (21400000 to 21450000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 430 (21450000 to 21500000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 431 (21500000 to 21550000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 432 (21550000 to 21600000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 433 (21600000 to 21650000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 434 (21650000 to 21700000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 435 (21700000 to 21750000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 436 (21750000 to 21800000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 437 (21800000 to 21850000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 438 (21850000 to 21900000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 439 (21900000 to 21950000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 440 (21950000 to 22000000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 441 (22000000 to 22050000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 442 (22050000 to 22100000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 443 (22100000 to 22150000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 444 (22150000 to 22200000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 445 (22200000 to 22250000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 446 (22250000 to 22300000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 447 (22300000 to 22350000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 448 (22350000 to 22400000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 449 (22400000 to 22450000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 450 (22450000 to 22500000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 451 (22500000 to 22550000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 452 (22550000 to 22600000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 453 (22600000 to 22650000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 454 (22650000 to 22700000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 455 (22700000 to 22750000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 456 (22750000 to 22800000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 457 (22800000 to 22850000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 458 (22850000 to 22900000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 459 (22900000 to 22950000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 460 (22950000 to 23000000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 461 (23000000 to 23050000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 462 (23050000 to 23100000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 463 (23100000 to 23150000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 464 (23150000 to 23200000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 465 (23200000 to 23250000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 466 (23250000 to 23300000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 467 (23300000 to 23350000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 468 (23350000 to 23400000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 469 (23400000 to 23450000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 470 (23450000 to 23500000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 471 (23500000 to 23550000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 472 (23550000 to 23600000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 473 (23600000 to 23650000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 474 (23650000 to 23700000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 475 (23700000 to 23750000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 476 (23750000 to 23800000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 477 (23800000 to 23850000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 478 (23850000 to 23900000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 479 (23900000 to 23950000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 480 (23950000 to 24000000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 481 (24000000 to 24050000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 482 (24050000 to 24100000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 483 (24100000 to 24150000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 484 (24150000 to 24200000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 485 (24200000 to 24250000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 486 (24250000 to 24300000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 487 (24300000 to 24350000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 488 (24350000 to 24400000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 489 (24400000 to 24450000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 490 (24450000 to 24500000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 491 (24500000 to 24550000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 492 (24550000 to 24600000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 493 (24600000 to 24650000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 494 (24650000 to 24700000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 495 (24700000 to 24750000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 496 (24750000 to 24800000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 497 (24800000 to 24850000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 498 (24850000 to 24900000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 499 (24900000 to 24950000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 500 (24950000 to 25000000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 501 (25000000 to 25050000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 502 (25050000 to 25100000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 503 (25100000 to 25150000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 504 (25150000 to 25200000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 505 (25200000 to 25250000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 506 (25250000 to 25300000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 507 (25300000 to 25350000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 508 (25350000 to 25400000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 509 (25400000 to 25450000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 510 (25450000 to 25500000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 511 (25500000 to 25550000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 512 (25550000 to 25600000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 513 (25600000 to 25650000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 514 (25650000 to 25700000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 515 (25700000 to 25750000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 516 (25750000 to 25800000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 517 (25800000 to 25850000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 518 (25850000 to 25900000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 519 (25900000 to 25950000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 520 (25950000 to 26000000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 521 (26000000 to 26050000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 522 (26050000 to 26100000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 523 (26100000 to 26150000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 524 (26150000 to 26200000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 525 (26200000 to 26250000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 526 (26250000 to 26300000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 527 (26300000 to 26350000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 528 (26350000 to 26400000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 529 (26400000 to 26450000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 530 (26450000 to 26500000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 531 (26500000 to 26550000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 532 (26550000 to 26600000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 533 (26600000 to 26650000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 534 (26650000 to 26700000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 535 (26700000 to 26750000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 536 (26750000 to 26800000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 537 (26800000 to 26850000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 538 (26850000 to 26900000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 539 (26900000 to 26950000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 540 (26950000 to 27000000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 541 (27000000 to 27050000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 542 (27050000 to 27100000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 543 (27100000 to 27150000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 544 (27150000 to 27200000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 545 (27200000 to 27250000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 546 (27250000 to 27300000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 547 (27300000 to 27350000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 548 (27350000 to 27400000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 549 (27400000 to 27450000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 550 (27450000 to 27500000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 551 (27500000 to 27550000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 552 (27550000 to 27600000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 553 (27600000 to 27650000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 554 (27650000 to 27700000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 555 (27700000 to 27750000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 556 (27750000 to 27800000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 557 (27800000 to 27850000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 558 (27850000 to 27900000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 559 (27900000 to 27950000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 560 (27950000 to 28000000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 561 (28000000 to 28050000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 562 (28050000 to 28100000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 563 (28100000 to 28150000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 564 (28150000 to 28200000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 565 (28200000 to 28250000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 566 (28250000 to 28300000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 567 (28300000 to 28350000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 568 (28350000 to 28400000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 569 (28400000 to 28450000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 570 (28450000 to 28500000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 571 (28500000 to 28550000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 572 (28550000 to 28600000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 573 (28600000 to 28650000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 574 (28650000 to 28700000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 575 (28700000 to 28750000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 576 (28750000 to 28800000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 577 (28800000 to 28850000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 578 (28850000 to 28900000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 579 (28900000 to 28950000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 580 (28950000 to 29000000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 581 (29000000 to 29050000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 582 (29050000 to 29100000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 583 (29100000 to 29150000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 584 (29150000 to 29200000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 585 (29200000 to 29250000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 586 (29250000 to 29300000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 587 (29300000 to 29350000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 588 (29350000 to 29400000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 589 (29400000 to 29450000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 590 (29450000 to 29500000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 591 (29500000 to 29550000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 592 (29550000 to 29600000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 593 (29600000 to 29650000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 594 (29650000 to 29700000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 595 (29700000 to 29750000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 596 (29750000 to 29800000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 597 (29800000 to 29850000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 598 (29850000 to 29900000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 599 (29900000 to 29950000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 600 (29950000 to 30000000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 601 (30000000 to 30050000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 602 (30050000 to 30100000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 603 (30100000 to 30150000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 604 (30150000 to 30200000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 605 (30200000 to 30250000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 606 (30250000 to 30300000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 607 (30300000 to 30350000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 608 (30350000 to 30400000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 609 (30400000 to 30450000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 610 (30450000 to 30500000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 611 (30500000 to 30550000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 612 (30550000 to 30600000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 613 (30600000 to 30650000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 614 (30650000 to 30700000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 615 (30700000 to 30750000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 616 (30750000 to 30800000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 617 (30800000 to 30850000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 618 (30850000 to 30900000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 619 (30900000 to 30950000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 620 (30950000 to 31000000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 621 (31000000 to 31050000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 622 (31050000 to 31100000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 623 (31100000 to 31150000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 624 (31150000 to 31200000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 625 (31200000 to 31250000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 626 (31250000 to 31300000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 627 (31300000 to 31350000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 628 (31350000 to 31400000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 629 (31400000 to 31450000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 630 (31450000 to 31500000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 631 (31500000 to 31550000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 632 (31550000 to 31600000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 633 (31600000 to 31650000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 634 (31650000 to 31700000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 635 (31700000 to 31750000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 636 (31750000 to 31800000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 637 (31800000 to 31850000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 638 (31850000 to 31900000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 639 (31900000 to 31950000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 640 (31950000 to 32000000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 641 (32000000 to 32050000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 642 (32050000 to 32100000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 643 (32100000 to 32150000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 644 (32150000 to 32200000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 645 (32200000 to 32250000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 646 (32250000 to 32300000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 647 (32300000 to 32350000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 648 (32350000 to 32400000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 649 (32400000 to 32450000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 650 (32450000 to 32500000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 651 (32500000 to 32550000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 652 (32550000 to 32600000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 653 (32600000 to 32650000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 654 (32650000 to 32700000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 655 (32700000 to 32750000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 656 (32750000 to 32800000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 657 (32800000 to 32850000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 658 (32850000 to 32900000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 659 (32900000 to 32950000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 660 (32950000 to 33000000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 661 (33000000 to 33050000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 662 (33050000 to 33100000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 663 (33100000 to 33150000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 664 (33150000 to 33200000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 665 (33200000 to 33250000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 666 (33250000 to 33300000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 667 (33300000 to 33350000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 668 (33350000 to 33400000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 669 (33400000 to 33450000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 670 (33450000 to 33500000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 671 (33500000 to 33550000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 672 (33550000 to 33600000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 673 (33600000 to 33650000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 674 (33650000 to 33700000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 675 (33700000 to 33750000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 676 (33750000 to 33800000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 677 (33800000 to 33850000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 678 (33850000 to 33900000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 679 (33900000 to 33950000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 680 (33950000 to 34000000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 681 (34000000 to 34050000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 682 (34050000 to 34100000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 683 (34100000 to 34150000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 684 (34150000 to 34200000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 685 (34200000 to 34250000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 686 (34250000 to 34300000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 687 (34300000 to 34350000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 688 (34350000 to 34400000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 689 (34400000 to 34450000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 690 (34450000 to 34500000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 691 (34500000 to 34550000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 692 (34550000 to 34600000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 693 (34600000 to 34650000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 694 (34650000 to 34700000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 695 (34700000 to 34750000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 696 (34750000 to 34800000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 697 (34800000 to 34850000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 698 (34850000 to 34900000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 699 (34900000 to 34950000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 700 (34950000 to 35000000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 701 (35000000 to 35050000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 702 (35050000 to 35100000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 703 (35100000 to 35150000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 704 (35150000 to 35200000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 705 (35200000 to 35250000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 706 (35250000 to 35300000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 707 (35300000 to 35350000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 708 (35350000 to 35400000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 709 (35400000 to 35450000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 710 (35450000 to 35500000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 711 (35500000 to 35550000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 712 (35550000 to 35600000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 713 (35600000 to 35650000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 714 (35650000 to 35700000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 715 (35700000 to 35750000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 716 (35750000 to 35800000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 717 (35800000 to 35850000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 718 (35850000 to 35900000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 719 (35900000 to 35950000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 720 (35950000 to 36000000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 721 (36000000 to 36050000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 722 (36050000 to 36100000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 723 (36100000 to 36150000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 724 (36150000 to 36200000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 725 (36200000 to 36250000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 726 (36250000 to 36300000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 727 (36300000 to 36350000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 728 (36350000 to 36400000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 729 (36400000 to 36450000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 730 (36450000 to 36500000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 731 (36500000 to 36550000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 732 (36550000 to 36600000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 733 (36600000 to 36650000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 734 (36650000 to 36700000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 735 (36700000 to 36750000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 736 (36750000 to 36800000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 737 (36800000 to 36850000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 738 (36850000 to 36900000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 739 (36900000 to 36950000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 740 (36950000 to 37000000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 741 (37000000 to 37050000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 742 (37050000 to 37100000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 743 (37100000 to 37150000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 744 (37150000 to 37200000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 745 (37200000 to 37250000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 746 (37250000 to 37300000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 747 (37300000 to 37350000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 748 (37350000 to 37400000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 749 (37400000 to 37450000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 750 (37450000 to 37500000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 751 (37500000 to 37550000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 752 (37550000 to 37600000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 753 (37600000 to 37650000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 754 (37650000 to 37700000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 755 (37700000 to 37750000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 756 (37750000 to 37800000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 757 (37800000 to 37850000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 758 (37850000 to 37900000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 759 (37900000 to 37950000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 760 (37950000 to 38000000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 761 (38000000 to 38050000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 762 (38050000 to 38100000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 763 (38100000 to 38150000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 764 (38150000 to 38200000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 765 (38200000 to 38250000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 766 (38250000 to 38300000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 767 (38300000 to 38350000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 768 (38350000 to 38400000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 769 (38400000 to 38450000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 770 (38450000 to 38500000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 771 (38500000 to 38550000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 772 (38550000 to 38600000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 773 (38600000 to 38650000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 774 (38650000 to 38700000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 775 (38700000 to 38750000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 776 (38750000 to 38800000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 777 (38800000 to 38850000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 778 (38850000 to 38900000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 779 (38900000 to 38950000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 780 (38950000 to 39000000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 781 (39000000 to 39050000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 782 (39050000 to 39100000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 783 (39100000 to 39150000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 784 (39150000 to 39200000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 785 (39200000 to 39250000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 786 (39250000 to 39300000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 787 (39300000 to 39350000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 788 (39350000 to 39400000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 789 (39400000 to 39450000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 790 (39450000 to 39500000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 791 (39500000 to 39550000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 792 (39550000 to 39600000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 793 (39600000 to 39650000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 794 (39650000 to 39700000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 795 (39700000 to 39750000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 796 (39750000 to 39800000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 797 (39800000 to 39850000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 798 (39850000 to 39900000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 799 (39900000 to 39950000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 800 (39950000 to 40000000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 801 (40000000 to 40050000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 802 (40050000 to 40100000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 803 (40100000 to 40150000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 804 (40150000 to 40200000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 805 (40200000 to 40250000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 806 (40250000 to 40300000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 807 (40300000 to 40350000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 808 (40350000 to 40400000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 809 (40400000 to 40450000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 810 (40450000 to 40500000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 811 (40500000 to 40550000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 812 (40550000 to 40600000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 813 (40600000 to 40650000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 814 (40650000 to 40700000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 815 (40700000 to 40750000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 816 (40750000 to 40800000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 817 (40800000 to 40850000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 818 (40850000 to 40900000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 819 (40900000 to 40950000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 820 (40950000 to 41000000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 821 (41000000 to 41050000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 822 (41050000 to 41100000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 823 (41100000 to 41150000 rows)\n",
      "Uploaded 50000 rows to taxi_df\n",
      "Uploading chunk 824 (41150000 to 41169720 rows)\n",
      "Uploaded 19720 rows to taxi_df\n"
     ]
    }
   ],
   "source": [
    "start_row = uploaded_rows  # Start from the last uploaded row\n",
    "chunk_size = 50000  # Adjust chunk size if needed\n",
    "\n",
    "for i in range(start_row, len(taxi_df), chunk_size):\n",
    "    chunk = taxi_df.iloc[i:i+chunk_size]\n",
    "    chunk_num = i // chunk_size + 1\n",
    "    print(f\"Uploading chunk {chunk_num} ({i} to {min(i+chunk_size, len(taxi_df))} rows)\")\n",
    "    copy_df_to_postgres(chunk, 'taxi_df', cursor)\n",
    "    conn.commit()  # Commit after each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command executed successfully.\n",
      "Command executed successfully.\n",
      "Command executed successfully.\n",
      "Command executed successfully.\n"
     ]
    }
   ],
   "source": [
    "# List of SQL commands to execute\n",
    "sql_commands = [\n",
    "    \"\"\"\n",
    "    CREATE TABLE borough_lut_temp AS\n",
    "    SELECT DISTINCT * FROM borough_lut;\n",
    "\n",
    "    DROP TABLE borough_lut;\n",
    "\n",
    "    ALTER TABLE borough_lut_temp RENAME TO borough_lut;\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE contributing_factor_lut_temp AS\n",
    "    SELECT DISTINCT * FROM contributing_factor_lut;\n",
    "\n",
    "    DROP TABLE contributing_factor_lut;\n",
    "\n",
    "    ALTER TABLE contributing_factor_lut_temp RENAME TO contributing_factor_lut;\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE cross_street_lut_temp AS\n",
    "    SELECT DISTINCT * FROM cross_street_lut;\n",
    "\n",
    "    DROP TABLE cross_street_lut;\n",
    "\n",
    "    ALTER TABLE cross_street_lut_temp RENAME TO cross_street_lut;\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE off_street_lut_temp AS\n",
    "    SELECT DISTINCT * FROM off_street_lut;\n",
    "\n",
    "    DROP TABLE off_street_lut;\n",
    "\n",
    "    ALTER TABLE off_street_lut_temp RENAME TO off_street_lut;\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Execute each SQL command\n",
    "for command in sql_commands:\n",
    "    try:\n",
    "        cursor.execute(command)\n",
    "        conn.commit()\n",
    "        print(\"Command executed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing command: {e}\")\n",
    "        conn.rollback()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows removed successfully.\n",
      "Duplicate rows removed successfully.\n",
      "Error adding primary key: multiple primary keys for table \"collisions_df\" are not allowed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of SQL commands to remove duplicates for each table\n",
    "sql_remove_duplicates_commands = [\n",
    "    \"\"\"\n",
    "    DELETE FROM collisions_df\n",
    "    WHERE ctid NOT IN (\n",
    "        SELECT MIN(ctid)\n",
    "        FROM collisions_df\n",
    "        GROUP BY collision_id\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    DELETE FROM geometry_df\n",
    "    WHERE ctid NOT IN (\n",
    "        SELECT MIN(ctid)\n",
    "        FROM geometry_df\n",
    "        GROUP BY \"LocationID\"\n",
    "    );\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Execute each SQL command\n",
    "for command in sql_remove_duplicates_commands:\n",
    "    try:\n",
    "        cursor.execute(command)\n",
    "        conn.commit()\n",
    "        print(\"Duplicate rows removed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error removing duplicates: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "# Add primary keys to the tables\n",
    "sql_add_primary_keys = [\n",
    "    \"ALTER TABLE borough_lut ADD PRIMARY KEY (borough_id);\",\n",
    "    \"ALTER TABLE contributing_factor_lut ADD PRIMARY KEY (contributing_factor_id);\",\n",
    "    \"ALTER TABLE cross_street_lut ADD PRIMARY KEY (cross_street_id);\",\n",
    "    \"ALTER TABLE off_street_lut ADD PRIMARY KEY (off_street_id);\",\n",
    "    \"ALTER TABLE collisions_df ADD PRIMARY KEY (collision_id);\",\n",
    "    \"ALTER TABLE geometry_df ADD PRIMARY KEY (\\\"LocationID\\\");\",\n",
    "    \"ALTER TABLE taxi_df ADD COLUMN trip_id SERIAL PRIMARY KEY;\"\n",
    "]\n",
    "\n",
    "# Execute each SQL command to add primary keys\n",
    "for command in sql_add_primary_keys:\n",
    "    try:\n",
    "        cursor.execute(command)\n",
    "        conn.commit()\n",
    "        print(\"Primary key added successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error adding primary key: {e}\")\n",
    "        conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error adding foreign key: name 'cursor' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'conn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 44\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241m.\u001b[39mexecute(command)\n\u001b[1;32m     45\u001b[0m     conn\u001b[38;5;241m.\u001b[39mcommit()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cursor' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError adding foreign key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241m.\u001b[39mrollback()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conn' is not defined"
     ]
    }
   ],
   "source": [
    "# Add foreign key constraints\n",
    "sql_add_foreign_keys = [\n",
    "    \"\"\"\n",
    "    ALTER TABLE taxi_df\n",
    "    ADD CONSTRAINT fk_taxi_pickup_location\n",
    "    FOREIGN KEY (PULocationID)\n",
    "    REFERENCES geometry_df (LocationID);\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    ALTER TABLE taxi_df\n",
    "    ADD CONSTRAINT fk_taxi_dropoff_location\n",
    "    FOREIGN KEY (DOLocationID)\n",
    "    REFERENCES geometry_df (LocationID);\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    ALTER TABLE collisions_df\n",
    "    ADD CONSTRAINT fk_collision_borough\n",
    "    FOREIGN KEY (borough_id)\n",
    "    REFERENCES borough_lut (borough_id);\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    ALTER TABLE collisions_df\n",
    "    ADD CONSTRAINT fk_collision_contributing_factor\n",
    "    FOREIGN KEY (contributing_factor_id)\n",
    "    REFERENCES contributing_factor_lut (contributing_factor_id);\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    ALTER TABLE collisions_df\n",
    "    ADD CONSTRAINT fk_collision_cross_street\n",
    "    FOREIGN KEY (cross_street_id)\n",
    "    REFERENCES cross_street_lut (cross_street_id);\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    ALTER TABLE collisions_df\n",
    "    ADD CONSTRAINT fk_collision_off_street\n",
    "    FOREIGN KEY (off_street_id)\n",
    "    REFERENCES off_street_lut (off_street_id);\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Execute each SQL command to add foreign keys\n",
    "for command in sql_add_foreign_keys:\n",
    "    try:\n",
    "        cursor.execute(command)\n",
    "        conn.commit()\n",
    "        print(\"Foreign key added successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error adding foreign key: {e}\")\n",
    "        conn.rollback()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
